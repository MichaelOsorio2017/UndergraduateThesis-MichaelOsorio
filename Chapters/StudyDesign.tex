% Chapter Template

\chapter{Empirical Study} % Main chapter title

\label{Chapter4} % Change X to a consecutive number; for referencing this chapter elsewhere, use \ref{ChapterX}

%----------------------------------------------------------------------------------------
%	SECTION 1
%----------------------------------------------------------------------------------------
\section{Study Design}\label{sec:studydesign}

The aim of this paper is to provide information about some of the most used automatic exploration tools for android applications inside the industry and the academy. This information is going to be useful for developers and researchers when they face a decision-making situation related to the selection of the right exploration tool that fits their needs. In consequence, a empirical study was designed and will provide answers for the following research questions: 

\begin{itemize}
\item RQ-1 What tool reaches the highest average method coverage?
\item RQ-2 What tool reaches the highest accumulated method coverage?
\item RQ-3 What tool finds the highest number of failures during the explorations?
\item RQ-4 what tool has the highest average of found errors?
\end{itemize}

\section{Context of the Study}

With the purpose of answering the research questions, 11 applications where selected to be executed. The list of selected apps can be seen in Figure.\ref{tab:apps}. This set is a subset of a set of open source applications utilised inside The Software Design Lab research group for other studies and tests, including RIP. Every APK in the subset should be successfully instrumented by InstruAPK, it should compile without any problem after instrumentation and it should be launch in an emulator without any issue after the instrumentation process.

Equally important, four exploration tools where selected, two from the industry and two from the academic side. The first tool was Firebase Test Lab (Section \ref{sec:testlab}). it was selected for being widely used in industry and for also being a Google product. The second one, Monkey \footnote{https://developer.android.com/studio/test/monkey}, was selected for being the most basic one. It is by default included in the Android SDK Tools. The third one, Droidbot (Section \ref{sec:droidbot}), was selected from the academic side. Droidbot has been a point of study for many researches. Many others tools have based their functionality on this tool. The last one is RIP (Section \ref{sec:rip}), this tool was selected for being of special interest for us. It is our own exploration tool and is is currently an active project inside the Software Design Lab at University of Los Andes. 

Every tool was executed ten times per application, and every execution with a maximum time of 30 minutes. Some tools ended its exploration before the max time. 
The number of executions and the maximum time were arbitrary decisions that were made because of time limitations for the study. Although, during the study was notice that most of the tools ended the exploration or reached their maximum coverage within the first 15 minutes. Which means that the maximum time for exploration was more than enough in almost all cases. 

The same emulator was used for all the exploration tools, in exception to Firebase Test Lab because this tool offers its own set of emulators. In all the cases a Pixel 2 XL was used, but in the cases of Monkey, Droidbot and RIP there was more control over the device specifications.For the case of the last mentioned tools the specifications are: Google APIs Intel Atom (x86), API level 27, SD card size of 512MB, and RAM size of 4096MB. This specifications are unknown for the case of Test Lab.

\begin{table}[t]
	\centering
	\caption{Applications used for the study}
	\label{tab:apps}
	\resizebox{0.80\textwidth}{!}{
		\begin{tabular}{c c c c}
			App ID & Package Name & \# Methods Reported by APKAnalyzer & \# Methods Instrumented by InstruAPK\\
			\hline
			1 & appinventor.ai\_nels0n0s0ri0.MiRutina & 61993 & 9351\\
			2 & com.evancharlton.mileage & 4000 & 1162\\
			3 & com.fsck.k9 & 18799 & 7003\\
			4 & com.ichi2.anki & 32370 & 2209\\
			5 & com.workingagenda.devinettes & 19274 & 66 \\
			6 & de.vanitasvitae.enigmandroid & 13083 & 574 \\
			7 & info.guardianproject.ripple & 19429 & 100 \\
			8 & org.connectbot & 20606 & 1145\\
			9 & org.gnucash.android & 75473 & 504\\
			10 & org.libreoffice.impressremote & 14691 & 649\\
			11 & org.lumicall.android & 45784 & 540\\
			\hline
		\end{tabular}
	}
\end{table}

Finally, the work flow specified in Section \ref{sec:generalApproach} was follow for every application using the named exploration tools. 

\section{Results}\label{sec:results}

\subsection{Method Coverage Results}\label{sec:coverageResults}

As mentioned before, some explorations ended before the max execution time allowed (30 minutes). giving no data for the upcoming seconds. To solve this scenarios, the coverage reached by the tool in the second the exploration ends, was kept the same until complete the total time. Once this have been done, the results are comparable second by second.


As a result of the instrumentation made by InstruAPK, the timestamp of every called method is know, so that, the coverage reached by a tool in a specific second can be calculated. The aforementioned information was used to calculate the average method coverage reached for every tool. Such results are presented in figure \ref{fig:averageCoverageInstruAPK} and figure \ref{fig:averageCoverageAPKAnalyzer}. The data were calculated as follow: 

\begin{enumerate}
\item For every exploration of an application add the coverage reached for tool second by second without adding the results of the previous seconds.
\item Divide by the number of explorations by tool. (This results in the average method coverage reached by a tool in one application)
\item Add the results of the previous step. (By tool)
\item Divide by the number of applications (This will end in the average method coverage of a tool for all the applications)
\end{enumerate} 

For calculate this, both data was used, the method coverage according InstruAPK and APKAnalyzer. The curves, have the same behaviour, as expected, but in Figure \ref{fig:averageCoverageAPKAnalyzer} the gap between tools is more visible.

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../Figures/averageCoverageInstruAPK.pdf}
\caption{Average Method Coverage by Tool According InstruAPK}\label{fig:averageCoverageInstruAPK}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../Figures/averageCoverageAPKAnalyzer.pdf}
\caption{Average Method Coverage by Tool According APKAnalyzer}\label{fig:averageCoverageAPKAnalyzer}
\end{figure}

The answer for RQ-1 can be seen easily in figure \ref{fig:averageCoverageInstruAPK} or figure \ref{fig:averageCoverageAPKAnalyzer}. Firebase Test Lab is the tool with the highest average method coverage reached. Surprisingly, followed by Monkey, even when monkey has no complex architecture nor exploration strategy, it has the second highest average method coverage within this study.

For answering RQ-2 is know that, for every execution the called methods are stored, every method has an unique id that was given during the instrumentation. Thus, after the 10 executions of an application, the result of every execution is analysed searching for the methods that have not been called neither during previous explorations nor during the current one. Therefore, the number of unique methods called in that application is obtained. That is what is named as accumulated coverage. This results can be seen in figure \ref{fig:boxplotAccumulated}. The coverage presented in figure \ref{fig:boxplotAccumulated} was calculated using the number of instrumented methods reported by InstruAPK. 

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../Figures/boxplotAccumulated.pdf}
\caption{Boxplot of Accumulated Coverage by Tool}\label{fig:boxplotAccumulated}
\end{figure}

So, again, the tool at first place is Firebase Test Lab, but this time, the second tool with the highest value is Droidbot.

\subsection{Error Results}\label{sec:errorResults}

Two different values for the errors were calculated. first, the total number of different error traces found by a tool throughout all the explorations. That is what is shown in figure \ref{fig:maxerrors}. The second one, show in figure \ref{fig:averagaerrors}, is the average number of errors found by tool throughout all the explorations. 

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../Figures/maxErrors.pdf}
\caption{Maximum Number of Errors Found by Tool}\label{fig:maxerrors}
\end{figure}

\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{../Figures/averageErrors.pdf}
\caption{Average Number of Errors Found by Tool}\label{fig:averagaerrors}
\end{figure}

This numbers were extracted by CoverageAnalyzer (Section \ref{sec:ca}). The number includes android runtime exceptions as well as exceptions. The tool searches for stack traces in the logcat file and filters the results using the package name of the tool under analysis. 

This data give the responses for RQ-3 and RQ-4. Which in both cases is Monkey.

